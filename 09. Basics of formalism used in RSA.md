# The utility Function used in RSA

$\text{Utility}'(u, s) = \text{ln} (P_{L_0}(s \mid u)) - \text{Cost}'(u)$

--- 

### Additivity of Information 

The logarithm is **intimately connected to information theory**, where it measures **information content** or **surprisal**. 

**Surprisal** = $-\ln P_{L_0}(s \mid u)$

- A **higher probability** $\(P_{L_0}(s \mid u)\)$ means **less surprisal** (less information is gained because the outcome is predictable).
- A **lower probability** means **more surprisal** (more information is gained because the outcome is unexpected).

Using $\(\ln P_{L_0}(s \mid u)\)$ in the utility function means the speaker is reasoning in terms of **information gain**, emphasizing how much information the utterance conveys to the listener.

#### Additivity

Logarithms turn probabilities (which are multiplicative) into additive values. This simplifies calculations in recursive reasoning. This additivity aligns with how information accumulates across multiple utterances, making it easier to combine contributions from multiple factors.

- If two events occur **independently**, their joint probability is $P(A, B) = P(A) \cdot P(B)$.
- Taking the log makes this additive: $\ln P(A, B) = \ln P(A) + \ln P(B)$.

---

### Emphasizing Relative Differences

Logarithms emphasize **relative differences** between probabilities, particularly when they are small. 

- The difference between \(P = 0.9\) and \(P = 0.8\) (large probabilities) is less impactful than the difference between \(P = 0.2\) and \(P = 0.1\) (small probabilities).
- Using \(\log\) reflects this intuition, as differences in smaller probabilities are stretched, giving them greater weight.

This makes \(\log P_{L_0}(s \mid u)\) more sensitive to **informative but less likely utterances**, which might play a critical role in communication.

---

### 3. **Behavioral Intuition: Diminishing Returns**
Logarithms reflect **diminishing returns**, which aligns with human intuition about utility. For example:
- If the probability of an event increases from \(0.01\) to \(0.1\), the increase in informativeness is substantial.
- If the probability increases from \(0.8\) to \(0.9\), the informativeness increase is much smaller.

This diminishing returns property ensures that the utility function doesn't overemphasize small changes in already-high probabilities. It captures the fact that once an utterance is very informative, there's less incentive to make it even more precise.

---

### 4. **Compatibility with Probabilistic Frameworks**
The RSA framework is inherently probabilistic, and \(\log\) fits naturally into such models for several reasons:
- **Bayesian Reasoning:** In Bayesian inference, likelihoods and priors are often combined multiplicatively. Taking the logarithm turns this into an additive process, simplifying the math.
- **Entropy and KL Divergence:** Many probabilistic models rely on measures like entropy and Kullback-Leibler (KL) divergence, both of which involve logarithms. For instance:
  - Entropy: \( H = -\sum P(x) \log P(x) \)
  - KL Divergence: \( D_{\text{KL}}(P \parallel Q) = \sum P(x) \log \frac{P(x)}{Q(x)} \)

By using \(\log P_{L_0}(s \mid u)\), RSA is consistent with these broader probabilistic frameworks.

---

### 5. **Scaling and Numerical Stability**
Logarithms help manage scaling issues in probabilities:
- Probabilities are often small numbers (e.g., \(P \approx 0.0001\)). Directly working with such small values can lead to numerical instability or computational issues (e.g., underflow in computer calculations).
- Taking the logarithm converts these small probabilities into more manageable values. For example:
  - \( P = 0.0001 \implies \log P \approx -9.21 \)
  - \( P = 0.9 \implies \log P \approx -0.105 \)

This transformation keeps calculations stable and interpretable.

---

### 6. **Intuition: "More Probable = More Utility"**
By taking \(\log P_{L_0}(s \mid u)\), the utility function directly captures the intuition that **more probable interpretations** are more useful to the listener. However, it also ensures that:
- **Very low-probability interpretations** are not entirely ignored (they still contribute negatively to the utility).
- **Moderate probabilities** are appropriately weighted, avoiding overly skewed behavior toward extreme values.

---

### Example: Comparing Utterances
Suppose a speaker has two possible utterances for describing a state \(s\), and the literal listener assigns the following probabilities:

- \( P_{L_0}(s \mid u_1) = 0.9 \)
- \( P_{L_0}(s \mid u_2) = 0.1 \)

If the speaker considers these probabilities directly, \( u_1 \) is 9 times more likely than \( u_2 \). However, using \(\log\), the informativeness differences look like this:

- \( \log P_{L_0}(s \mid u_1) \approx -0.105 \)
- \( \log P_{L_0}(s \mid u_2) \approx -2.302 \)

Now, the difference between \( u_1 \) and \( u_2 \) is **additive**, not multiplicative, and it reflects the fact that \( u_2 \) (though unlikely) could still provide substantial information if chosen.

---

### Summary of Motivations for Using \(\log\):
1. **Captures Information Content:** Reflects the surprisal of events in a way consistent with information theory.
2. **Additivity:** Makes combining multiple probabilities easier by turning products into sums.
3. **Relative Emphasis:** Highlights differences in small probabilities, making the model sensitive to low-probability but informative utterances.
4. **Diminishing Returns:** Models how informativeness increases more significantly at lower probabilities.
5. **Numerical Stability:** Handles small probabilities without computational issues.
6. **Theoretical Consistency:** Aligns with broader probabilistic and Bayesian frameworks.

Would you like an example of how this works in RSA, or how it compares to not using \(\log\)?
