# Informativeness in RSA

$\text{Utility}'(u, s) = \text{ln} (P_{L_0}(s \mid u)) - \text{Cost}'(u)$

--- 

### Additivity of Information 

The logarithm is **intimately connected to information theory**, where it measures **information content** or **surprisal**. 

**Surprisal** = $-\ln P_{L_0}(s \mid u)$

- A **higher probability** $\(P_{L_0}(s \mid u)\)$ means **less surprisal** (less information is gained because the outcome is predictable).
- A **lower probability** means **more surprisal** (more information is gained because the outcome is unexpected).

Using $\ln P_{L_0}(s \mid u)$ in the utility function means the speaker is reasoning in terms of **information gain**, emphasizing how much information the utterance conveys to the listener.

#### Additivity

Logarithms turn probabilities (which are multiplicative) into additive values. This simplifies calculations in recursive reasoning. This additivity aligns with how information accumulates across multiple utterances, making it easier to combine contributions from multiple factors.

- If two events occur **independently**, their joint probability is $P(A, B) = P(A) \cdot P(B)$.
- Taking the log makes this additive: $\ln P(A, B) = \ln P(A) + \ln P(B)$.

**Example: Computing Natural Log (ln) Probabilities of a Sentence**

Consider the sentence "The cat sleeps."

Using a bigram language model, we assume the following estimated probabilities:

- $P(\text{"The"}) = 0.3$
- $P(\text{"cat"} | \text{"The"}) = 0.5$
- $P(\text{"sleeps"} | \text{"cat"}) = 0.4$
- $P(\text{"."} | \text{"sleeps"}) = 0.8$

The probability of the entire sequence is:

$P(\text{"The cat sleeps."}) = P(\text{"The"}) \times P(\text{"cat"} | \text{"The"}) \times P(\text{"sleeps"} | \text{"cat"}) \times P(\text{"."} | \text{"sleeps"})$

Substituting the values:

$P(\text{"The cat sleeps."}) = 0.3 \times 0.5 \times 0.4 \times 0.8 = 0.048$

Then, we take the natural logarithm (ln) of the probability:

$\ln P(\text{"The cat sleeps."}) = \ln(0.3) + \ln(0.5) + \ln(0.4) + \ln(0.8)$

Approximating the natural logarithm values:

$\ln(0.3) \approx -1.204, \quad \ln(0.5) \approx -0.693, \quad \ln(0.4) \approx -0.916, \quad \ln(0.8) \approx -0.223$

Summing these:

$-1.204 + (-0.693) + (-0.916) + (-0.223) = -3.036$

The natural log probability of "The cat sleeps." is -3.036. Since ln probabilities are always negative (because probabilities are between 0 and 1), a value closer to zero indicates a **more probable** sequence.

---

### Relative differences & diminishing returns

Logarithms emphasize **relative differences** between probabilities, particularly when they are small. 

- The difference between $P(u) = 0.9$ and $P(u) = 0.8$ (large probabilities) represent high-likelihood events, so their difference feels less important in terms of informativeness compared to the difference between $P(u) = 0.2$ and $P(u) = 0.1$ (small probabilities), which represent low-likelihood events.
- Using $\ln$ reflects this intuition, as differences in smaller probabilities are stretched, giving them greater weight.

  - For $P(u) = 0.9$, $-\ln(0.9) \approx 0.105$; for $P(u) = 0.8$, $-\ln(0.8) \approx 0.223$; the difference between $P(u) = 0.9$ and $P(u) = 0.8$ is $(0.223 - 0.105) = 0.118$
  - For $P(u) = 0.2$, $-\ln(0.2) \approx 1.609$; for $P(u) = 0.1$, $-\ln(0.1) \approx 2.302$; the difference between $P(u) = 0.9$ and $P(u) = 0.8$ is $(2.302 - 1.609) = 0.693$

Logarithms reflect **diminishing returns**, which aligns with human intuition about utility. 

- If the probability of an event increases from \(0.01\) to \(0.1\), the increase in informativeness is substantial.
- If the probability increases from \(0.8\) to \(0.9\), the informativeness increase is much smaller.

In the RSA framework, this property is crucial because:

- High-probability utterances: Convey less new information and are less valuable in distinguishing between possible situations.
- Low-probability utterances: May be surprising but are often more informative, as they narrow down the listenerâ€™s interpretation more effectively.

  > **Example**: Suppose you are live in Singapore, ... <br>
  > $u_1 =$ 'it will rain today': $P(u_1) = 0.8$. Hearing this adds little new information since you already expect rain. <br>
  > $u_2 =$ 'It will snow today': $P(u_2) = 0.1$. Hearing this is much more surprising and informative. 

This makes $\ln P_{L_0}(s \mid u)$ more sensitive to **informative but less likely utterances**, which might play a critical role in communication. The diminishing returns property ensures that the utility function doesn't overemphasize small changes in already-high probabilities. It captures the fact that once an utterance is very informative, there's less incentive to make it even more precise.

---

### Summary of using $\ln(P)$:
1. **Captures Information Content:** Reflects the surprisal of events in a way consistent with information theory.
2. **Additivity:** Makes combining multiple probabilities easier by turning products into sums.
3. **Relative Emphasis:** Highlights differences in small probabilities, making the model sensitive to low-probability but informative utterances.
4. **Diminishing Returns:** Models how informativeness increases more significantly at lower probabilities.

