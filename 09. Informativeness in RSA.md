# Informativeness in RSA

$\text{Utility}'(u, s) = \text{ln} (P_{L_0}(s \mid u)) - \text{Cost}'(u)$

--- 

### Additivity of Information 

The logarithm is **intimately connected to information theory**, where it measures **information content** or **surprisal**. 

**Surprisal** = $-\ln P_{L_0}(s \mid u)$

- A **higher probability** $\(P_{L_0}(s \mid u)\)$ means **less surprisal** (less information is gained because the outcome is predictable).
- A **lower probability** means **more surprisal** (more information is gained because the outcome is unexpected).

Using $\ln P_{L_0}(s \mid u)$ in the utility function means the speaker is reasoning in terms of **information gain**, emphasizing how much information the utterance conveys to the listener.

#### Additivity

Logarithms turn probabilities (which are multiplicative) into additive values. This simplifies calculations in recursive reasoning. This additivity aligns with how information accumulates across multiple utterances, making it easier to combine contributions from multiple factors.

- If two events occur **independently**, their joint probability is $P(A, B) = P(A) \cdot P(B)$.
- Taking the log makes this additive: $\ln P(A, B) = \ln P(A) + \ln P(B)$.

---

### **Example: Probability Updates with Natural Logarithm (ln)**

**Scenario: A Listener Interpreting a Speaker's Intent** <br>

Imagine a speaker describes an object in a scene where the target object is a **blue square**, and the listener is reasoning about its identity.

**Step 1**: The listener starts with a **prior probability** for each object being the referent:

| Object        | Prior Probability \( P(s) \) |
|--------------|--------------------|
| **Blue Square**  | 0.5 |
| **Blue Circle**  | 0.3 |
| **Red Square**  | 0.2 |

Taking the **natural logarithm (ln)** of these probabilities gives **log-probabilities**, which are **additive**:

| Object        | $ln P(s)$ |
|--------------|--------------|
| **Blue Square**  | $\ln 0.5 = -0.693$ |
| **Blue Circle**  | $\ln 0.3 = -1.204$ |
| **Red Square**  | $\ln 0.2 = -1.609$ |

**Step 2:**  The speaker says **"Blue"**, and the listener updates their belief about the referent.

Likelihood of Speaker choosing "Blue" given each object: 

| Object        | $P(u = \text{"Blue"} \mid s)$ |
|--------------|----------------------------|
| **Blue Square**  | 0.8 |
| **Blue Circle**  | 0.7 |
| **Red Square**  | 0.1 |

Instead of multiplying probabilities, we convert them into **ln-space** (natural logarithm turns multiplication into addition):  

| Object        | \( \ln P(w \mid s) \) |
|--------------|------------------|
| **Blue Square**  | \( \ln 0.8 = -0.223 \) |
| **Blue Circle**  | \( \ln 0.7 = -0.357 \) |
| **Red Square**  | \( \ln 0.1 = -2.303 \) |

Now, we **add** these ln-values to the prior ln-probabilities:

| Object        | Updated ln-Probability |
|--------------|------------------|
| **Blue Square**  | \( -0.693 + (-0.223) = -0.916 \) |
| **Blue Circle**  | \( -1.204 + (-0.357) = -1.561 \) |
| **Red Square**  | \( -1.609 + (-2.303) = -3.912 \) |

After exponentiating and normalizing, the updated probabilities are:

| Object        | Updated Probability |
|--------------|------------------|
| **Blue Square**  | 0.54 |
| **Blue Circle**  | 0.40 |
| **Red Square**  | 0.06 |

---

##### **Step 3: Second Utterance â€“ "Square"**  
Now, the speaker follows up with **"Square"**, further refining the listenerâ€™s understanding.

**Likelihoods for "Square":**

| Object        | \( P(w = \text{"Square"} \mid s) \) |
|--------------|----------------------------|
| **Blue Square**  | 0.9 |
| **Blue Circle**  | 0.1 |
| **Red Square**  | 0.8 |

Converted to **ln-space**:

| Object        | \( \ln P(w \mid s) \) |
|--------------|------------------|
| **Blue Square**  | \( \ln 0.9 = -0.105 \) |
| **Blue Circle**  | \( \ln 0.1 = -2.303 \) |
| **Red Square**  | \( \ln 0.8 = -0.223 \) |

Adding these to the previously updated ln-values:

| Object        | Final ln-Probability |
|--------------|------------------|
| **Blue Square**  | \( -0.916 + (-0.105) = -1.021 \) |
| **Blue Circle**  | \( -1.561 + (-2.303) = -3.864 \) |
| **Red Square**  | \( -3.912 + (-0.223) = -4.135 \) |

After exponentiating and normalizing, we get:

| Object        | Final Probability |
|--------------|------------------|
| **Blue Square**  | 0.88 |
| **Blue Circle**  | 0.08 |
| **Red Square**  | 0.04 |

---

### **Why This Works**  

âœ… **Using ln (natural logarithm) makes probability updates additive rather than multiplicative.**  
âœ… **Each new utterance contributes an additive update, simplifying recursive reasoning.**  
âœ… **This mirrors how conversational context accumulates over time, allowing the listener to refine their interpretation.**  

---

This example now explicitly uses **ln (natural logarithm, base e)** rather than general "log" notation. Would you like me to reattempt a visualization for this? ðŸ˜Š

---

### Relative differences & diminishing returns

Logarithms emphasize **relative differences** between probabilities, particularly when they are small. 

- The difference between $P(u) = 0.9$ and $P(u) = 0.8$ (large probabilities) represent high-likelihood events, so their difference feels less important in terms of informativeness compared to the difference between $P(u) = 0.2$ and $P(u) = 0.1$ (small probabilities), which represent low-likelihood events.
- Using $\ln$ reflects this intuition, as differences in smaller probabilities are stretched, giving them greater weight.

  - For $P(u) = 0.9$, $-\ln(0.9) \approx 0.105$; for $P(u) = 0.8$, $-\ln(0.8) \approx 0.223$; the difference between $P(u) = 0.9$ and $P(u) = 0.8$ is $(0.223 - 0.105) = 0.118$
  - For $P(u) = 0.2$, $-\ln(0.2) \approx 1.609$; for $P(u) = 0.1$, $-\ln(0.1) \approx 2.302$; the difference between $P(u) = 0.9$ and $P(u) = 0.8$ is $(2.302 - 1.609) = 0.693$

Logarithms reflect **diminishing returns**, which aligns with human intuition about utility. 

- If the probability of an event increases from \(0.01\) to \(0.1\), the increase in informativeness is substantial.
- If the probability increases from \(0.8\) to \(0.9\), the informativeness increase is much smaller.

In the RSA framework, this property is crucial because:

- High-probability utterances: Convey less new information and are less valuable in distinguishing between possible situations.
- Low-probability utterances: May be surprising but are often more informative, as they narrow down the listenerâ€™s interpretation more effectively.

  > **Example**: Suppose you are live in Singapore, ... <br>
  > $u_1 =$ 'it will rain today': $P(u_1) = 0.8$. Hearing this adds little new information since you already expect rain. <br>
  > $u_2 =$ 'It will snow today': $P(u_2) = 0.1$. Hearing this is much more surprising and informative. 

This makes $\ln P_{L_0}(s \mid u)$ more sensitive to **informative but less likely utterances**, which might play a critical role in communication. The diminishing returns property ensures that the utility function doesn't overemphasize small changes in already-high probabilities. It captures the fact that once an utterance is very informative, there's less incentive to make it even more precise.

---

### Summary of using $\ln(P)$:
1. **Captures Information Content:** Reflects the surprisal of events in a way consistent with information theory.
2. **Additivity:** Makes combining multiple probabilities easier by turning products into sums.
3. **Relative Emphasis:** Highlights differences in small probabilities, making the model sensitive to low-probability but informative utterances.
4. **Diminishing Returns:** Models how informativeness increases more significantly at lower probabilities.

